# RAG-Based Question Answering System

A production-ready Retrieval-Augmented Generation (RAG) system that enables semantic search and question answering over uploaded documents.

## Features

- ðŸ“„ **Multi-format Support**: PDF and TXT document processing
- ðŸ” **Hybrid Search**: BM25 (Keyword) + FAISS (Semantic) for optimal retrieval
- ðŸ–¥ï¸ **Web Interface**: User-friendly HTML/JS frontend for easy interaction
- ðŸ³ **Docker Support**: Containerized for consistent deployment
- ðŸ§  **Semantic Search**: Vector-based similarity search
- ðŸ¤– **LLM-powered Answers**: Google Gemini Flash for intelligent answer generation
- âš¡ **Async Processing**: Background document processing with FastAPI
- ðŸ“Š **Metrics Tracking**: Comprehensive latency and quality metrics
- ðŸ”’ **Rate Limiting**: Built-in API rate limiting (10 req/min)
- âœ… **Type Validation**: Pydantic models for request/response validation

## Architecture

```mermaid
graph TD
    User[User] -->|Uploads PDF/TXT| API[FastAPI Server]
    User -->|Asks Question| API
    
    subgraph "Backend System"
        API -->|Extracts Text| Processor[Document Processor]
        Processor -->|Chunks Text| Processor
        Processor -->|Generates Embeddings| VectorStore[Vector Store (FAISS)]
        
        API -->|Search Query| VectorStore
        VectorStore -->|Retrieves Chunks| API
        
        API -->|Context + Question| LLM[LLM Service (Google Gemini)]
        LLM -->|Generates Answer| API
    end
    
    VectorStore -->|Persists Data| Disk[Local Storage]
```

## System Flow

```
1. Document Upload
   User uploads PDF/TXT â†’ FastAPI receives â†’ Background job created
   â†“
2. Processing (Background)
   Extract text â†’ Chunk into 512-token segments â†’ Generate embeddings
   â†“
3. Storage
   Embeddings â†’ FAISS index | Metadata â†’ JSON/Pickle | Status tracking
   â†“
4. Query Processing
   User question â†’ Generate query embedding â†’ FAISS similarity search
   â†“
5. Answer Generation
   Retrieved chunks â†’ Build context â†’ Claude generates answer â†’ Return with sources
   â†“
6. Metrics
   Track latency, similarity scores, confidence â†’ Store for analysis
```

## Installation

### Prerequisites

- Python 3.9+
- pip

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd rag-qa-system
```

2. Create virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Set up environment variables:
```bash
# Optional: Set Claude API key for real LLM responses
export ANTHROPIC_API_KEY="your-api-key-here"

# If not set, system will use mock responses
```

5. Run the server:
```bash
python app.py
```

The API will be available at `http://localhost:8000`

## API Documentation

### 1. Upload Document

**Endpoint**: `POST /upload`

**Description**: Upload a document for processing

**Supported Formats**: PDF, TXT

**Request**:
```bash
curl -X POST "http://localhost:8000/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"
```

**Response**:
```json
{
  "document_id": "550e8400-e29b-41d4-a716-446655440000",
  "filename": "document.pdf",
  "status": "processing",
  "message": "Document uploaded and processing started"
}
```

### 2. Ask Question

**Endpoint**: `POST /ask`

**Description**: Ask a question based on uploaded documents

**Request Body**:
```json
{
  "question": "What is the main topic discussed in the document?",
  "document_id": "550e8400-e29b-41d4-a716-446655440000",  // Optional
  "top_k": 5  // Optional, default: 5
}
```

**Request Example**:
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is quantum computing?",
    "top_k": 5
  }'
```

**Response**:
```json
{
  "answer": "According to Source 1, quantum computing is...",
  "source_chunks": [
    {
      "text": "Quantum computing uses quantum bits...",
      "filename": "quantum.pdf",
      "chunk_index": 3,
      "score": 0.87,
      "document_id": "550e8400-..."
    }
  ],
  "confidence_score": 0.82,
  "latency_ms": 645.23,
  "document_ids": ["550e8400-..."]
}
```

### 3. Check Document Status

**Endpoint**: `GET /documents/{document_id}`

**Request**:
```bash
curl "http://localhost:8000/documents/550e8400-e29b-41d4-a716-446655440000"
```

**Response**:
```json
{
  "document_id": "550e8400-e29b-41d4-a716-446655440000",
  "filename": "document.pdf",
  "status": "completed",
  "chunks_count": 42,
  "processed_at": "2024-01-27T10:30:00"
}
```

### 4. List Documents

**Endpoint**: `GET /documents`

**Request**:
```bash
curl "http://localhost:8000/documents"
```

**Response**:
```json
[
  {
    "document_id": "550e8400-...",
    "filename": "document1.pdf",
    "status": "completed",
    "chunks_count": 42,
    "processed_at": "2024-01-27T10:30:00"
  },
  {
    "document_id": "660e8400-...",
    "filename": "document2.txt",
    "status": "processing",
    "chunks_count": null,
    "processed_at": null
  }
]
```

### 5. Get Metrics

**Endpoint**: `GET /metrics`

**Description**: Get system performance metrics

**Request**:
```bash
curl "http://localhost:8000/metrics"
```

**Response**:
```json
{
  "total_queries": 150,
  "latency": {
    "avg_ms": 678.45,
    "min_ms": 234.12,
    "max_ms": 1523.67,
    "p95_ms": 1100.23,
    "p99_ms": 1450.89
  },
  "retrieval": {
    "avg_time_ms": 123.45,
    "avg_similarity": 0.756
  },
  "llm": {
    "avg_time_ms": 534.21
  },
  "confidence": {
    "avg": 0.782,
    "min": 0.234,
    "max": 0.967
  },
  "quality_metrics": {
    "high_confidence_queries": 120,
    "low_similarity_queries": 12,
    "slow_queries_over_1s": 8
  }
}
```

### 6. Delete Document

**Endpoint**: `DELETE /documents/{document_id}`

**Request**:
```bash
curl -X DELETE "http://localhost:8000/documents/550e8400-e29b-41d4-a716-446655440000"
```

**Response**:
```json
{
  "message": "Document deleted successfully",
  "document_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

### 7. Health Check

**Endpoint**: `GET /health`

**Request**:
```bash
curl "http://localhost:8000/health"
```

**Response**:
```json
{
  "status": "healthy",
  "timestamp": "2024-01-27T10:30:00",
  "documents_count": 5
}
```

## Interactive API Documentation

FastAPI provides automatic interactive documentation:

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

## Usage Examples

### Example 1: Upload and Query

```python
import requests
import time

# 1. Upload document
with open('research_paper.pdf', 'rb') as f:
    response = requests.post(
        'http://localhost:8000/upload',
        files={'file': f}
    )
document_id = response.json()['document_id']

# 2. Wait for processing
time.sleep(5)

# 3. Check status
status = requests.get(f'http://localhost:8000/documents/{document_id}')
print(status.json())

# 4. Ask question
answer = requests.post(
    'http://localhost:8000/ask',
    json={
        'question': 'What are the main findings?',
        'document_id': document_id,
        'top_k': 5
    }
)
print(answer.json()['answer'])
```

### Example 2: Multi-document Query

```python
# Upload multiple documents
doc_ids = []
for file in ['doc1.pdf', 'doc2.txt', 'doc3.pdf']:
    with open(file, 'rb') as f:
        response = requests.post(
            'http://localhost:8000/upload',
            files={'file': f}
        )
        doc_ids.append(response.json()['document_id'])

# Wait for processing
time.sleep(10)

# Query across all documents (don't specify document_id)
answer = requests.post(
    'http://localhost:8000/ask',
    json={
        'question': 'Compare the methodologies used',
        'top_k': 10
    }
)

print(f"Answer: {answer.json()['answer']}")
print(f"Sources: {answer.json()['document_ids']}")
```

## Rate Limiting

The API implements rate limiting:
- **Limit**: 10 requests per minute per IP address
- **Response**: 429 Too Many Requests if exceeded

## Project Structure

```
rag-qa-system/
â”œâ”€â”€ app.py                    # Main FastAPI application
â”œâ”€â”€ document_processor.py     # Text extraction and chunking
â”œâ”€â”€ vector_store.py           # FAISS vector store management
â”œâ”€â”€ llm_service.py           # Claude LLM integration
â”œâ”€â”€ metrics_tracker.py       # Performance metrics tracking
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ DESIGN_DECISIONS.md      # Technical decisions explained
â”œâ”€â”€ README.md               # This file
â”‚
â”œâ”€â”€ uploads/                # Temporary uploaded files
â”œâ”€â”€ vector_store/          # Persisted FAISS index and metadata
â”‚   â”œâ”€â”€ faiss.index
â”‚   â”œâ”€â”€ chunks_metadata.pkl
â”‚   â””â”€â”€ document_metadata.json
â””â”€â”€ metrics/               # Query metrics logs
    â””â”€â”€ query_metrics.json
```

## Configuration

### Chunking Parameters (document_processor.py)

```python
CHUNK_SIZE = 512      # tokens (~400 words)
CHUNK_OVERLAP = 50    # tokens (10% overlap)
```

### Embedding Model (vector_store.py)

```python
MODEL_NAME = "all-MiniLM-L6-v2"  # 384-dimensional embeddings
```

### Rate Limiting (app.py)

```python
RATE_LIMIT = 10          # requests per minute
RATE_LIMIT_WINDOW = 60   # seconds
```

## Technical Details

### Chunking Strategy

- **Size**: 512 tokens (~400 words, ~2000 characters)
- **Method**: Semantic-aware chunking that respects paragraph boundaries
- **Overlap**: 50 tokens to prevent information loss at boundaries
- See `DESIGN_DECISIONS.md` for detailed rationale

### Vector Store

- **Engine**: FAISS (Facebook AI Similarity Search)
- **Index Type**: IndexFlatIP (Inner Product / Cosine Similarity)
- **Embedding Model**: sentence-transformers/all-MiniLM-L6-v2
- **Persistence**: Automatic save to disk after operations

### LLM Integration

- **Model**: Claude Sonnet 4.5
- **Max Tokens**: 1024 for answer generation
- **Context**: Top-k retrieved chunks with metadata
- **Fallback**: Mock responses when API key not set

## Performance

### Typical Latencies (on standard hardware)

- **Document Upload**: < 100ms (immediate response, processing in background)
- **Document Processing**: 2-5 seconds per page
- **Query (Retrieval)**: 50-150ms
- **Query (LLM)**: 500-1500ms
- **Total Query**: < 2 seconds (P95)

### Scalability

- **Documents**: Tested with 100+ documents
- **Chunks**: Handles 10,000+ chunks efficiently
- **Query Throughput**: 10-20 queries/second (with caching)

## Monitoring

The system tracks:
- **Latency**: Total, retrieval, and LLM time
- **Similarity Scores**: Average relevance of retrieved chunks
- **Confidence**: Combined quality metric
- **Quality Indicators**: High/low confidence query rates

Access metrics via `GET /metrics` endpoint.

## Troubleshooting

### Issue: "ANTHROPIC_API_KEY not set" warning

**Solution**: 
```bash
export ANTHROPIC_API_KEY="your-key"
```
Or use mock mode (automatic fallback)

### Issue: Slow query responses

**Possible Causes**:
- Large number of documents (>1000 chunks)
- Slow internet for Claude API calls
- CPU-bound embedding generation

**Solutions**:
- Use GPU for embeddings (install `faiss-gpu`)
- Reduce `top_k` parameter
- Add caching layer

### Issue: Low similarity scores

**Possible Causes**:
- Query and document vocabulary mismatch
- Documents don't contain relevant information

**Solutions**:
- Check query phrasing
- Upload more relevant documents
- Consider hybrid search (keyword + vector)

## Testing

Run basic tests:

```bash
# Start server
python app.py

# In another terminal
curl http://localhost:8000/health

# Upload test document
curl -X POST "http://localhost:8000/upload" \
  -F "file=@test_document.txt"

# Query
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{"question": "test question", "top_k": 3}'
```

## Design Decisions

See `DESIGN_DECISIONS.md` for detailed explanations of:
- Why 512 token chunks were chosen
- Observed retrieval failure cases
- Metrics selection and rationale
- Technology choices

## Future Enhancements

- [ ] Hybrid search (vector + BM25 keyword search)
- [ ] Cross-encoder re-ranking
- [ ] Multi-hop reasoning support
- [ ] Document versioning
- [ ] User authentication
- [ ] Query expansion
- [ ] Feedback loop integration

## License

MIT License

## Contributing

Contributions welcome! Please open an issue or submit a pull request.

## Contact

For questions or issues, please open a GitHub issue.
